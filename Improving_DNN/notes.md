# Notes

### November 17, 2019 (started)
Had a lot of chores to do (as well as picking up some stuff I left at the apartment) so I didn't get as much time in as I wanted today. But that's alright, I've been going past time a lot the past few days :)

#### Accomplished Today
- Learned about
    + Strategies for reducing bias and variance with NNs
        * variance can be reduced with regularization and increasing the amount of training data you have
        * bias can be reduced by increasing the complexity of your NN
    + Methods for regularization
        * L1 and L2 norms
        * Dropout regularization
    + Methods for speeding up training
        * Exploding and dissapearing weights (due to the depth of nets)
            - Reducing the variance in the initial set of parameters
        * Normalization (to help gradient descent behave)
    + Using numerical approximation of gradients to check for bugs in your backwards propogation
- Completed all the lectures for week 1 of the course
#####2hrs

#### Goals for Next Time
- Complete the quiz
- Complete the practice problems
- Complete week 1!

### November 18, 2019

#### Accomplished Today
- Finished up week 1!
- Saw just how dramatic initialization can be on your final results
- Also saw how dramatic regularization can be
Pretty cool stuff
#####1hr

#### Goals for Next Time
- Week 2 lectures
- Maybe the assignments too? (given they are so quick)

### November 19, 2019

#### Accomplished Today
- Learned about various ways to speed up learning:
    + RMSProp
    + Momentum
    + Minibatch
    + Adams
    + Learning rate decay

#### Goals for Next Time
- Finish week 2!
- Finish week 3 lectures
#####1.5hrs

### November 20, 2019

#### Accomplished Today
- Finished week 2!
- Learned about some new things:
    + Batch normalization
    + Hyperparameter tuning
        * Random searching 
        * Scaling hyperparameter searches properly

#### Goals for Next Time
- Finish week 3!
